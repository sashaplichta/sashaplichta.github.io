---
layout: default
modal-id: 203
date: 2024-01-01
img: labelled_cellvit.png
alt: image-alt
project-date:
where:
category:
description: |
    CellViT is a vision transformer (the original ViT paper is called "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale") is a nuclei segmentation and classification model trained on hematoxylin and eosin (H&E) stained tissue samples found in the PanNuke dataset. H&E stains are used by pathologists for a wide variety of diagnostics and is especially useful in oncology. Digital pathology is pretty broad, but a fundamental tenet is training segmentation and classification models to compliment the diagnostic work of human pathologists. There have been a lot of successful models developed over the years (the original U-Net was designed for biomedical imaging), but in recent years convolutional neural networks (CNNs) have been favored since they're fairly translationally invariant - the same convolutions are applied across the whole image, so it shouldn't matter too much if something interesting is in the top left corner or the bottom right. One of the big hang-ups for ViT models is that they don't have this translational invariance, so they may need much larger datasets to match the performance of CNNs, but past this threshold can exceed them. CellViT, for example, achieves state-of-the-art performance on the MoNuSeg dataset without fine tuning. So, how does CellViT (and ViTs in general) work?

    # First - A word about transformers

    I won't go into too much detail here (check out my other post on Transformers), but there are a couple things worth reiterating before diving into the specifics of CellViT. First, transformers work by mapping tokens to embeddings and back. This means that to implement a transformer, you need to have some way of generating a vocabulary of tokens - pretty easy for a discrete space like language (we can do something like Byte-Pair-Encoding), but much more difficult for an almost continuous space like RGB images. Also, transformers can only deal with a linear input - there's no notion of a 2d structure like the x and y coordinates of images.

    # CellViT

    ## Model Basics

    Ok, so we've established that ViTs work, so how do they overcome the fundamental challenges of using a discrete, 1d architecture to model a (pretty much) continuous, 2d (or 3d if you want to count color) data space? First, lets deal with tokenization. In the original ViT paper, they use a 16 x 16 selection as the base unit, then create a vocabulary by clustering all the selections in the training dataset into $$V$$ clusters, where $$V$$ is the size of the vocabulary. That is to say, they take each image in the training dataset, generate all possible 16 x 16 slices to generate their "corpus", and then use a clustering algorithm to generate a vocabulary. Then, during inference, each 16 x 16 tile is mapped to the closest vocabulary token and embedded with a single linear layer. So, discrete/continuous issue overcome, but what about our dimensionality constraint? It turns out this is pretty easily overcome by just flattening the 16 x 16 patches comprising an image (remember, for transformers, the token input also has a corresponding spatial input). Once that's done, we've turned our continuous, 2d input into a tokenized, 1d representation usable by the transformer. A final note about the input - the creators of CellViT also prepend a learnable class token to for classification tasks. So, overall we know that the CellViT model takes an image, tokenizes and flattens it, and somehow creates a semantic segmentation. Let's take a closer look at what the model is acutally doing.

    ## Model Architecture
    The model is based around a U-Net architecture with an encoder, decoder, and skip connections connecting the two. One of the main motivations for the authors' use of a ViT-based model was to enable the use of large, pre-trained encoders like Meta AI's Segment Anything Model (SAM). CellViT uses five skip connections to give the decoder access to granular information, improving segmentation performance, especially at boundaries between classes. The decoder is formed of successive steps of deconvolution to increase the resolution and fusion with the skip connections. Finally, three segmentation branches predict: a binary segmentation map (NP branch), a distance map (HV branch), and a nuclei type map (NT branch). A hybrid loss function inclusive of each head is used to speed up training and convergence, and takes the form:

    $$ \mathcal{L}_{total} = \mathcal{L}_{NP} + \mathcal{L}_{HV} + \mathcal{L}_{NT} + \mathcal{L}_{TC} $$

    Each of the first three terms refers to the loss at each of the heads described above, while TC refers to a Tissue Classification head used to guide the encoder's learning process. I won't go into the exact definition of each loss function here, but broadly: 
    - the NP loss is comprised of a Dice score (measures similarity between nuclei segmentations) and Feature Token (FT, refines the patch embeddings) loss 
    - the HV loss uses the Mean Squared Error (MSE, measures the similarity between predicted distances and true distances) and Mean Squared Gradient Error (MSGE - apparently this encourages smoothness in the prediction)
    - the NT loss is derived from FT, Dice, and Binary Cross Entropy (BCE, measures how close the model's class probabilities align with the true labels) losses
    -  the TC loss is just Cross Entropy loss 
    
    Since the inference is performed by tiling and stitching together patches of gigabyte-sized whole slide images (WSI), the authors relied heavily on post-processing to stitch together the outputs of each head into a cohesive and useful prediction. In particular, they used an edge-detection filter followed by a marker-controlled watershed algorithm to establish boundaries between nuclei segmentations, then assigned nuclei type labels to each segmentation using majority voting in the nuclei type map. That is to say, each segmentation was assigned the label that corresponded to the majority of its component pixels.

    Finally, the authors compared the performance of two pre-trained encoders ($$ViT_{256}$$ and $$SAM$$) against a model trained with randomly initialized weights and tested the model's generalizability by evaluating its performance on the MoNuSeg H&E image dataset. They found that when using the HoVer-Net (another segmentation digital pathology model) decoder, the CellViT encoder was able to perform similarly to the end-to-end HoVer-Net. When using a pre-trained encoder and learning the decoder, however, CellViT was able to achieve state-of-the-art performance. This is particularly interesting because it implies that the image encodings learned by models like SAM (trained on general image data) can be readily applied to specialized fields, like digital pathology, that were likely under-represented in the original training dataset. This supports the use of generalist foundation models as the basis for specialized model training.

---